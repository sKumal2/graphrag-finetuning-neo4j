# ğŸš€ GraphRAG Fine-Tuning Environment - Setup Complete!

## What Has Been Created

Your complete fine-tuning environment is ready with:

### âœ… Core Components
- **fine_tune.py** - Main fine-tuning trainer with backward compatibility
- **multi_agent_orchestration.py** - 5 specialized agents + supervisor orchestrator
- **data_loaders.py** - FireRisk dataset loader + utilities
- **requirements.txt** - All dependencies included

### âœ… Startup Scripts
- **finetune_setup.py** - Complete environment setup (Run First!)
- **start_finetuning.py** - Begin fine-tuning with one command
- **test_setup.py** - Verify everything works

### âœ… Documentation
- **SETUP_GUIDE.md** - Comprehensive setup & usage guide
- **MULTI_AGENT_ARCHITECTURE.md** - Architecture deep-dive
- **FINETUNING_README.md** - Quick reference
- **QUICK_REFERENCE.py** - Interactive guide

### âœ… Dataset
- **FireRisk** - Remote sensing fire risk classification
  - 91,872 images total
  - 7 classes (fire risk levels)
  - Loading 5,000 samples for quick start
  - Converted to text documents for embedding fine-tuning

---

## ğŸ¯ 3-Step Quick Start

### Step 1: Setup Environment
```bash
python finetune_setup.py
```
This will:
- âœ“ Install dependencies
- âœ“ Download FireRisk dataset
- âœ“ Create project directories
- âœ“ Generate startup scripts

### Step 2: Verify Installation
```bash
python test_setup.py
```
This will:
- âœ“ Test dataset loading
- âœ“ Verify configuration
- âœ“ Check all systems

### Step 3: Start Fine-Tuning
```bash
python start_finetuning.py
```
This will:
- âœ“ Execute multi-agent pipeline
- âœ“ Train embedding classifier
- âœ“ Generate visualizations

---

## ğŸ“Š Multi-Agent Architecture

### 5 Specialized Agents + Supervisor

```
INPUT: Documents â†’ SUPERVISOR â†’ OUTPUT: Model + Metrics

Agents Executed in Sequence:

1. DataPrepAgent
   â””â”€ Split & balance data
   
2. RetrieverConfigAgent
   â””â”€ Setup embeddings & vectorstore
   
3. TrainingAgent
   â””â”€ Train classification head
   
4. EvaluationAgent
   â””â”€ Test & compute metrics
   
5. ReportingAgent
   â””â”€ Generate visualizations
```

### Key Features
- âœ“ Modular & testable design
- âœ“ Clear orchestration flow
- âœ“ Automatic checkpointing
- âœ“ Comprehensive metrics
- âœ“ Beautiful visualizations
- âœ“ Result persistence (JSON)

---

## ğŸ—‚ï¸ Project Structure

```
New folder/
â”œâ”€â”€ Core Modules
â”‚   â”œâ”€â”€ fine_tune.py
â”‚   â”œâ”€â”€ multi_agent_orchestration.py
â”‚   â”œâ”€â”€ data_loaders.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Startup Scripts
â”‚   â”œâ”€â”€ finetune_setup.py          â† RUN FIRST
â”‚   â”œâ”€â”€ start_finetuning.py
â”‚   â””â”€â”€ test_setup.py
â”‚
â”œâ”€â”€ Documentation
â”‚   â”œâ”€â”€ SETUP_GUIDE.md             â† READ THIS
â”‚   â”œâ”€â”€ QUICK_REFERENCE.py
â”‚   â”œâ”€â”€ MULTI_AGENT_ARCHITECTURE.md
â”‚   â””â”€â”€ FINETUNING_README.md
â”‚
â””â”€â”€ Runtime (Created Automatically)
    â”œâ”€â”€ data/firerisk/              # Dataset cache
    â”œâ”€â”€ checkpoints/                # Model checkpoints
    â”œâ”€â”€ agent_outputs/              # Agent results
    â””â”€â”€ logs/                        # Training logs
```

---

## âš™ï¸ Configuration Required

### 1. Update .env File

Create/edit `.env` with your API keys:

```env
# Required: Google API (for embeddings)
GOOGLE_API_KEY=your_google_api_key

# Optional: Chroma (for cloud storage)
CHROMA_API_KEY=your_key
CHROMA_TENANT=default

# Optional: HuggingFace (for datasets)
HUGGINGFACE_TOKEN=your_token
```

### 2. Training Config (Optional)

Edit `start_finetuning.py` to customize:

```python
config = get_default_config()
config['epochs'] = 30           # Number of epochs
config['batch_size'] = 32       # Batch size
config['learning_rate'] = 3e-4  # Learning rate
# ... more options
```

---

## ğŸ“ˆ Expected Output

After training, you'll get:

### Files Generated
```
agent_outputs/
â”œâ”€â”€ data_prep_results.json
â”œâ”€â”€ training_results.json
â”œâ”€â”€ evaluation_results.json
â””â”€â”€ training_report.png          â† Visualization

checkpoints/
â””â”€â”€ best_model.pth               â† Best model
```

### Metrics
- **F1 Score** (weighted & macro)
- **Accuracy**
- **Precision** & **Recall**
- **AUC** (for binary classification)
- **Learning curves** (loss, accuracy, F1)

---

## ğŸ“ Dataset: FireRisk

### Overview
- **Source**: Hugging Face (blanchon/FireRisk)
- **Type**: Remote sensing fire risk classification
- **Size**: 91,872 images (using 5,000 for demo)
- **Classes**: 7 fire risk levels

### Classes
| Class | Risk Level |
|-------|-----------|
| 0 | high |
| 1 | low |
| 2 | moderate |
| 3 | non-burnable |
| 4 | very_high |
| 5 | very_low |
| 6 | water |

### Data Format
```python
{
    'id': 'firerisk_train_0',
    'content': 'Fire risk level: high. Remote sensing image.',
    'label': 0,
    'metadata': {...}
}
```

---

## ğŸ”§ Technology Stack

### Deep Learning
- **PyTorch** 2.0+ - Deep learning framework
- **CUDA/CPU** - GPU acceleration (if available)

### NLP & Embeddings
- **LangChain** - LLM/embedding orchestration
- **Google Generative AI** - 768-dim embeddings
- **Chroma** - Vector database

### ML Utils
- **scikit-learn** - Metrics & utilities
- **numpy** - Numerical computing
- **matplotlib** - Visualization

### Data
- **HuggingFace Datasets** - Dataset loading
- **transformers** - Pre-trained models

---

## ğŸ“š Next Steps

### Immediate (Do Now)
1. âœ“ Read SETUP_GUIDE.md
2. âœ“ Run finetune_setup.py
3. âœ“ Run test_setup.py
4. âœ“ Run start_finetuning.py

### Integration (Next)
1. Integrate fine-tuned embeddings with GraphRAG
2. Use custom documents instead of FireRisk
3. Deploy model for inference

### Advanced (Later)
1. Hyperparameter optimization (Optuna)
2. Experiment tracking (W&B, MLflow)
3. Multi-GPU training
4. Model quantization (ONNX, GGUF)

---

## ğŸ†˜ Troubleshooting

### "Missing GOOGLE_API_KEY"
â†’ Add to .env and restart

### "CUDA out of memory"
â†’ Reduce batch_size in config

### "Dataset download failed"
â†’ Script uses mock data automatically

### Training too slow
â†’ Reduce epochs or batch_size

For detailed troubleshooting, see SETUP_GUIDE.md

---

## ğŸ“ Files Reference

| File | Purpose | When to Use |
|------|---------|-----------|
| finetune_setup.py | Complete setup | Run first, once |
| test_setup.py | Verify installation | After setup, before training |
| start_finetuning.py | Begin training | Main training script |
| SETUP_GUIDE.md | Comprehensive guide | Learn everything |
| QUICK_REFERENCE.py | Interactive guide | Quick lookup |
| fine_tune.py | Fine-tuning module | Advanced customization |
| data_loaders.py | Data utilities | Load different datasets |

---

## ğŸ‰ You're All Set!

Your GraphRAG fine-tuning environment is ready. 

### Next Command:
```bash
python finetune_setup.py
```

Then:
```bash
python test_setup.py
```

Finally:
```bash
python start_finetuning.py
```

**Happy fine-tuning! ğŸš€**

---

## ğŸ“– Documentation Index

- **SETUP_GUIDE.md** â† Start here for comprehensive guide
- **QUICK_REFERENCE.py** â† Run for interactive guide
- **MULTI_AGENT_ARCHITECTURE.md** â† Architecture deep-dive
- **FINETUNING_README.md** â† Usage examples
- **example_multi_agent_finetune.py** â† Code examples

---

Created with â¤ï¸ for GraphRAG Fine-Tuning
Multi-Agent Architecture inspired by [mootboard](https://github.com/kshitizregmi/mootboard)
Dataset: [FireRisk](https://huggingface.co/datasets/blanchon/FireRisk)
